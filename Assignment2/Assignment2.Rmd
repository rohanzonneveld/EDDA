---
title: "Assignment 2 - Group 27"
author: "Joost Driessen, Emma van Lipzig, Rohan Zonneveld"
output: pdf_document
date: "`r Sys.Date()`"
theme: sandstone 
highlight: tango
---

```{r setup, include=FALSE}
set.seed(111)
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
library(car)
library(glmnet)
library(readr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(glmnet)
```


# Excercise 1. Trees
## Intro
```{r exercise 1 intro, echo=FALSE}
mydata = read.table('treeVolume.txt')

mydata = mydata[-1, ]
names(mydata)[names(mydata) == "V1"] <- "diameter"
names(mydata)[names(mydata) == "V2"] <- "height"
names(mydata)[names(mydata) == "V3"] <- "volume"
names(mydata)[names(mydata) == "V4"] <- "type"
mydata$volume <- as.numeric(mydata$volume)
mydata$height <- as.numeric(mydata$height)
mydata$diameter <- as.numeric(mydata$diameter)

par(mfrow=c(1,2))
hist(mydata$volume, main="Histogram of Tree volume", xlab="Volume")
qqnorm(mydata$volume)

shapiro.test(mydata$volume)
```
The histogram is slightly skewed to the right and the QQ-plot looks normal. So, the plots give no reason to suspect the data is not drawn from a normal distribution. However the Shapiro-Wilk test does reject normality ($p<0.05$). However, since it is explicitly stated in the question that we should use anova we decided to use parametric tests throughout **Exercise 1**.

## a)
``` {r exercise 1A, echo=FALSE}
boxplot(volume~type, data = mydata, main = 'Volumes of different tree types')
```
```{r}
model1 = lm(volume~type, data = mydata)
anova(model1)

beechvolume = mydata$volume[1:31]
oakvolume = mydata$volume[32:59]
t.test(beechvolume, oakvolume)
```

The p-value from the ANOVA-test is 0.174, which is bigger than 0.05, so we can not reject
H0, thus there are equal variances between the volumes of the two tree types. A t-test can 
also be related to this result. The outcome of this test is that there is no difference between the mean volumes of the two types of trees. The mean volume for a beech is 30.17097 and for an oak it is 35.25.

## b)
```{r exercise 1B}
model2 = lm(volume~diameter*type, data = mydata)

model3 = lm(volume~height*type, data = mydata)

full_model = lm(volume~diameter * height * type, data = mydata)

anova(model2, full_model)
anova(model3, full_model)
```

The new ANOVA-tests show that both height and diameter significantly influence volume (p < 0.01). They also demonstrate that there is no interaction effect between diameter and type on volume, but also that there is an interaction effect between height and type on volume (p < 0.01).

## c)
```{r exercise 1C}
mydata$type = as.factor(mydata$type)
model4 = lm(volume~height + diameter + type, data = mydata)
drop1(model4, test = 'F')
model5 = lm(volume~height + diameter, data = mydata)
summary(model5)

df = data.frame(diameter=mean(mydata$diameter), height = mean(mydata$height))
predict(model5, df, interval = 'prediction')
```
All of the variables height, diameter and type have an influence on the volume of the tree,
which means it would be wrong to exclude one of those variables, per the previous models.
Model4 was constructed and drop1 was used to investigate whether all the used variables
have an impact on the model. It shows that type is not significant, so type is excluded 
from the new model. The model to predict the average tree volume now only includes diameter
and volume.

## d)
```{r exercise 1D}
mydata$newvolume = pi * (mydata$diameter/2)^2 * mydata$height
model6 = lm(volume ~ newvolume, data = mydata)
summary(model6)

full_model = lm(volume~height + diameter + newvolume, data =mydata)
omega1 = lm(volume~height + diameter, data = mydata)
omega2 = lm(volume~newvolume, data =mydata)

anova(omega1, full_model)
anova(omega2, full_model)
```
This linear model might not be the best one to explain volume in terms of diameter and height. Since the volume is the product of the surface area and it's height, it would make more sense to construct as follows: $volume = \pi * (diameter/2)^2 * height$. In this model the surface area of a circle (tree) is calculated using $\pi r^2$ and it's multiplied by it's height to calculate the volume. The adjusted R-squared of this new model is 0.9743 and the ANOVA between the two submodels and the full model shows that this new model is a better predictor than the old model from *c*.

# Excercise 2. Expenditure on criminal activities
## a)
```{r, fig.height=4, fig.width=8, echo=FALSE}
data=read.table(file="expensescrime.txt",header=TRUE)
data=data[,-1]
par(mfrow=c(1,3))
hist(data$expend, xlab='expenditure',main='Histogram')
boxplot(data$expend)
qqnorm(data$expend)
```

This dataset contains a number outliers.

### Influence Points
To investigate if these outliers are due to leverage points a scatter plot is made of the response variable expenditure against all individual explanatory variables. This plot also includes the linear regression model depicted as a blue line. Next, the residuals of a linear regression model containing the corresponding explanatory variable is plotted. Third, a QQ-plot of the residuals is plotted. Lastly, the Cook's distance is calculated and plotted with a line at cut-off value 1 depicted in red.

```{r, fig.height=20, fig.width=16, echo=FALSE}
par(mfrow=(c(5,4)))

model <- lm(expend~bad, data=data)
plot(data$bad,data$expend, 
main='Scatter expend,bad',xlab='bad',ylab='expend')
abline(model, col='blue')
plot(data$bad,residuals(model), main='Residuals vs. bad',xlab='bad',ylab='expend')
qqnorm(residuals(model), main='QQ-plot residuals')
plot(1:length(data$bad), cooks.distance(model), type='b', main="Cook's distances",xlab="index",ylab="Cook's distance")
abline(h=1, col="red")

model <- lm(expend~crime, data=data)
plot(data$crime,data$expend, 
main='Scatter expend,crime',xlab='crime',ylab='expend')
abline(model, col='blue')
plot(data$crime,residuals(model), main='Residuals vs. crime',xlab='crime',ylab='expend')
qqnorm(residuals(model), main='QQ-plot residuals')
plot(1:length(data$crime), cooks.distance(model), type='b', main="Cook's distances",xlab="index",ylab="Cook's distance")
abline(h=1, col="red")

model <- lm(expend~lawyers, data=data)
plot(data$lawyers,data$expend, 
main='Scatter expend,lawyers',xlab='lawyers',ylab='expend')
abline(model, col='blue')
plot(data$lawyers,residuals(model), main='Residuals vs. lawyers',xlab='lawyers',ylab='expend')
qqnorm(residuals(model), main='QQ-plot residuals')
plot(1:length(data$lawyers), cooks.distance(model), type='b', main="Cook's distances",xlab="index",ylab="Cook's distance")
abline(h=1, col="red")

model <- lm(expend~employ, data=data)
plot(data$employ,data$expend, 
main='Scatter expend,employ',xlab='employ',ylab='expend')
abline(model, col='blue')
plot(data$employ,residuals(model), main='Residuals vs. employ',xlab='employ',ylab='expend')
qqnorm(residuals(model), main='QQ-plot residuals')
plot(1:length(data$employ), cooks.distance(model), type='b', main="Cook's distances",xlab="index",ylab="Cook's distance")
abline(h=1, col="red")

model <- lm(expend~pop, data=data)
plot(data$pop,data$expend, 
main='Scatter expend,pop',xlab='pop',ylab='expend')
abline(model, col='blue')
plot(data$pop,residuals(model), main='Residuals vs. pop',xlab='pop',ylab='expend')
qqnorm(residuals(model), main='QQ-plot residuals')
plot(1:length(data$pop), cooks.distance(model), type='b', main="Cook's distances",xlab="index",ylab="Cook's distance")
abline(h=1, col="red")

```

The outliers are apparent in all plots. In the residuals plot most points concentrate around zero, the points with high values appear on the right side of the residuals plot, which means they are likely due to a leverage point. To test if these points are influence points the Cook's distance is calculated: 

$D_i=\frac{1}{(p+1) \hat{\sigma}^2} \sum\limits_{j=1}^n  ({\hat{Y}_{(i),j}-\hat{Y}_j})^2$. 

In words: the Cook's distance quantifies the influence of observation i on the prediction by calculating the sum of squared differences between the predicted values of the model with and without the i-th data point. In the last plot of every row the Cook's distance was plotted for all data points. From these plots we can conclude that all explanatory variables contain influence points except crime.

### Collinearity
To investigate the problem of collinearity pairwise plots are made and a correlation table is produced. 

```{r, echo=FALSE}
pairs(data)
```
```{r}
round(cor(data),2)
```
A lot of pairwise scatter plots show a linear pattern, which means these variables are probably collinear. Specifically, all explanatory variables are collinear except crime, which is clear from both the plot (data points scattered) and the correlation table ($R^2<0.80$). 

This approach was only capable of finding pairwise collinearities. To find multi-collinearity the Variance Inflation Factor ($VIF_j$) is calculated according to the following formula: 

$VIF_j=\frac{1}{1-R^2_j}$,

where $R^2_j$ the determination coefficient $R^2$ from the regression of the j-th explanatory $X_j$ (as response) variable on the remaining explanatory variables.

```{r}
model <- lm(expend~bad+crime+lawyers+employ+pop,data=data)
vif(model)
```
All values except crime are bigger than 5 so there is a collinearity problem, which was already clear from the plots and the table. 

## b)
We apply the step-up method to fit a linear regression model to the data. The first step is to find the variable that yields the maximum increase in $R^2$.

```{r}
model <- lm(expend~employ,data=data)
summary(model)[[4]];summary(model)[[8]]
```

In this case the variable that yields the highest increase in $R^2$ is employ. Employ is significant so we add it to the model and repeat the process with the starting model `lm(expend~employ)`.

```{r}
model <- lm(expend~employ+lawyers,data=data)
summary(model)[[4]];summary(model)[[8]]
```
 
Now the variable which leads to the highest increase in $R^2$ is lawyers. This variable is also significant so we add it to the model. Again the process is repeated, this time with starting model `lm(expend~employ+lawyers)`.

```{r}
model <- lm(expend~employ+lawyers+bad,data=data)
summary(model)[[4]];summary(model)[[8]]
```

```{r}
model <- lm(expend~employ+lawyers+pop,data=data)
summary(model)[[4]];summary(model)[[8]]
```

Both bad and pop lead to an $R^2$ of 0.964, however pop is not significant while bad is. Because the change in $R^2$ is really small and a model with fewer explanatory variables is commonly better looked upon the choice whether or not to add these variables is up for debate. We decided not to add one of these variable as it is not essential in explaining the expenditure. So, we stop the process and define `lm(expend~employ+lawyers)` as the model. Looking back to **a** it is clear that this model will have a problem with collinearity. Also the change in $R^2$ after the first iteration of the algorithm is negligible. We conclude that this is not a good model because it contains redundant information and it contains variables that do not contribute significantly in added $R^2$.

## c)
To find the 95% prediction interval for the expend of the hypothetical state, we perform the following r code:
```{r}
model <- lm(expend~employ+lawyers, data=data)
newxdata=data.frame(bad=50,crime=5000,lawyers=5000,employ=5000,pop=5000)
predict(model,newxdata,interval='prediction')
```

The interval is so big the lower confidence bound is negative, which is hard to interpret as expenditure is always a non-negative number. To improve this prediction interval we could lower the level or gather more data. However lowering the level would lead to a less weighty conclusion and gathering more data is not possible because there are a limited number of states. It is possible to find a better model with another method. If this model would explain more of the variance and have less errors the prediction interval would be smaller. Also, if a model is found with less regressors the degrees of freedom in the t-statistic would go up, that would lead to a smaller t-distribution and by result to a smaller prediction interval (more data points would have the same effect).

## d)
We apply the lasso method to find a model. First the data is divided in a train and test set (2:1). Then the model is found by minimizing an equation with the squared error between the train set and the fitted model subject to $\beta$. Also a penalty term is added  ($\sum_{k=0}^p \beta_k$), which increases linearly with a factor of $\lambda$. A plot of the $\beta$-weights against the parameter $\lambda$ is generated. Also, a plot is generated with the mean standard error of the model for every $\lambda$.

```{r, echo=FALSE}
x=as.matrix(data[,-1]) #remove the response variable
y=as.double(as.matrix(data[,1])) #only the response variable
train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train]  # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality
lasso.mod=glmnet(x.train,y.train,alpha=1)
lasso.cv=cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')

par(mfrow=c(1,2)) #TODO legenda
plot(lasso.mod,label=T,xvar="lambda")  #have a look at the lasso path
plot(lasso.cv) # the best lambda by cross-validation
```
Contrasting to what one might expect the $\lambda$ is chosen with the lowest mean standard error plus one standard deviation to penalize the $\beta$-weights even more and create an even simpler model. The lasso method with this $\lambda$ yield the following $\beta$-weights:

```{r, echo=FALSE}
lambda.1se=lasso.cv$lambda.1se
coef(lasso.mod,s=lambda.1se) #betaâ€™s for the best lambda
```
If the model with these $\beta$-weights is applied to predict the test set of the data the mean standard error equals:

```{r, echo=FALSE}
y.pred=predict(lasso.mod,s=lambda.1se,newx=x.test) #predict for test
mean((y.test-y.pred)^2)#mse for the predicted test rows
```

Calculating the same statistic for the model from **b** yields:
```{r, echo=FALSE}
y.pred=predict(model,newx=x.test)
mean((y.test-y.pred)^2)
```

The most important variables are included in both models (lawyers and employ) and the $\beta$-weights do not substantially differ from one another ($\approx0.01$). However, the mean squared error is significantly lower for the lasso model (up to tenfold). In conclusion, the lasso model is better able to find a good model although it does not substantially differ from the step-up model.

# Exercise 3. Titanic
## a)

```{r loading data titanic, echo=TRUE, message=FALSE, warning=FALSE}
data_titanic <- read.table('titanic.txt', header=TRUE)
data_titanic$Sex <- as.factor(data_titanic$Sex)
data_titanic$PClass <-as.factor(data_titanic$PClass)
```

The tables below show the percentage of passengers who survived per class and sex, per sex and per class. These tables show that both females, and first class passengers have a higher survival rate than males and second or third class passengers respectively. Both first and second class females have a particularly high survival rate. Because of these differences, Sex and PClass are expected to be important variables in a model that predicts survival. The boxplot shows the distribution of ages among survivors and non-survivors. These distributions do not appear very different, therefore Age is not not expected to be an important variable for a model that predicts survival.

```{r summarizing data titanic, echo=FALSE, warning=FALSE, fig.height=4}
taba <- xtabs(~Sex + PClass, data=data_titanic)
tabb <- xtabs(Survived~Sex + PClass, data=data_titanic)
round(tabb/taba,2) #table of survival rates over sex+class. 

tabc <- xtabs(~Sex, data=data_titanic)
tabd <- xtabs(Survived~Sex, data=data_titanic)
tabsex <- round(tabd/tabc,2);tabsex #table of survival rates over sex. 

tabe <- xtabs(~PClass, data=data_titanic)
tabf <- xtabs(Survived~PClass, data=data_titanic)
tabclass <- round(tabf/tabe,2);tabclass#table of survival rates over class. 

#boxplot of age distribution of survivors and nonsurvivors
boxplt_titanic_1 <- ggplot(data_titanic, aes(group = Survived, y = Age)) +         
  geom_boxplot(color="seagreen", fill="lightgreen") + labs(title = 'Age of survivors and non-survivors of the titanic', x = 'Survival status', y = 'Age')+ theme_light()

boxplt_titanic_1 
```

The linear regression model below shows that the odds for survival can be predicted approximately as exp{3.76 - 1.29SecondClass - 2.52ThirdClass - 2.63Sexmale - 0.04Age}. This means that the odds of survival are lower for second and third class passengers, male passengers. This is in line with the expectations from the tables. It also means the odds of survival decrease with age, which is an unexpected result. An example of odds predicted by this model would be the survival odds for a for a 30 year old second class male passenger, which are approximately 0.26.

```{r titanic logistic regression 1}
lg_titanic <- glm(Survived ~ PClass + Sex + Age,
                    data=data_titanic,family=binomial)
summary(lg_titanic)$coefficients[,c("Estimate","Pr(>|z|)")]
exponent <- 3.759662 -1.291962*1 -2.521419*0  -2.631357*1 -0.039177*30
exp(exponent) #survival odds 30yo 2nd class male
```

## b)

The models below show that there is a significant interaction effect between Age and Sex, but not between Age and PClass. Previous models showed that Age, PClass and Sex all have significant main effects. Therefore the resulting model includes PClass and an interaction between age and sex.

```{r titanic logistic regression interactions, echo=FALSE}
lg_class <- glm(Survived ~ Age * PClass,
                    data=data_titanic,family=binomial)
summary(lg_class)$coefficients[,c("Estimate","Pr(>|z|)")]

lg_sex <- glm(Survived ~ Age * Sex,
                    data=data_titanic,family=binomial)
summary(lg_sex)$coefficients[,c("Estimate","Pr(>|z|)")]

drop1(lg_class, test='Chisq')
drop1(lg_sex, test='Chisq')

lg_titanic2 <- glm(Survived ~ PClass + Age*Sex, 
                   data=data_titanic, family=binomial)

summary(lg_titanic2)$coefficients[,c("Estimate","Pr(>|z|)")]

```

The table below shows the estimates of the survival probability for a 55 year passenger of each sex in every class.

```{r titanic lgm predictions, echo=FALSE}
new_survival_probs <- data.frame(Sex=as.factor(c('female', 'female', 'female', 'male', 'male', 'male')), PClass=as.factor(c('1st', '2nd', '3rd','1st', '2nd', '3rd')), Age=55)
new_survival_probs$Predictions <- predict(lg_titanic2, new_survival_probs)
new_survival_probs
```

## c)

To predict the survival status of passengers, the logistic regression model from b) can be used to predict the estimated chance of survival for passengers with certain characteristics (age, sex and class). These estimates can then be turned into survival status predictions, by taking a survival chance of \<0.5 to be 'not survived' and a survival chance of \>0.5 as 'survived'. To test the quality of these predictions, k-cross validation can be used to test the accuracy of the predictions.

## d)

The contingency tables are based on values from the tables created in a). The number of people who did not survive are calculated with `taba-tabb` and `tabc-tabd`. To test the effect of the factor class on survival, a chi-square test is performed on a contingency table of all the first, second and third class passengers who did and did not survive the titanic. This test is significant, which indicates that class has a significant effect on how the survivors/non-survivors are distributed over the contingency table. the table below shows the difference between the observed and expected values of the class contingency table. In first and second class more people survived than expected if class did not have an effect, and in third class more people died than expected if class did not have an effect. Although this was expected for third class, the results for second class are more surprising because in the logistic regression model, second class was associated with a decrease in survival chance. This difference has likely arisen because in the logistic regression model, the factor class is treatment parametrized with first class as baseline. Compared to first class, second class passengers do have a lower survival chance, which is apparent in the logistic regression model. However, second class passengers still have a higher survival rate than the average survival rate, which is why more second class passengers survived than would be expected if class did not matter to survival. This becomes apparent in the contingency table chisquare test.

To test the effect of the factor sex, a fisher test is used instead, because this is a 2 by 2 contingency table for which an exact p-value can be computed. The test is significant, meaning that sex matters to survival rate. The odds ratio result can be interpeted as for every surviving male there are approximately ten surviving women.

```{r titanic contingency tables, echo=FALSE}
class_matrix <- matrix(c(193,119,138,129,161,573),
  byrow=TRUE,ncol=3,nrow=2, dimnames = 
  list(c("Survived","Died"),c("1st","2nd","3rd")))
sex_matrix <- matrix(c(308,142,154,709),
  byrow=TRUE,ncol=2,nrow=2, 
  dimnames = list(c("Survived","Died"),c("Female","Male")))

z_class <- chisq.test(class_matrix)
z_class
tabz_class <-round(z_class$expected,0)
class_matrix - tabz_class

z_sex <- fisher.test(sex_matrix)
z_sex

```

## e)

The approach in d) is not wrong. The results a contingency table approach more easily interpretable. However, this approach does not take into account the influence of different variables and their potential interaction effects on survival rate. If only the variables survival and sex or survival and class were available in the data, the contingency table approach would be suitable, but because more data is available, it is better to use this for more accurate picture of what did and did not affect the survival of titanic passengers.

# Excercise 4. Military coups
## a)
```{r, echo=FALSE}
data = read.table("coups.txt", header =TRUE)
```

We perform a poisson regression on the full dataset to create a model for the number of military coups. To find out what variables are significantly contributing to this model we compare a model without this variable to the full model. The output of this analysis is given below.

```{r}
data$pollib= as.factor(data$pollib)
 model=glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,family=poisson,data=data)
drop1(model, test = "Chisq")[5]
```
Oligarchy, pollib and parties have a significant effect on the number of military coups. To what extent these variables influence the response variable is given below.

```{r}
summary(model)$coefficients[,c("Estimate","Pr(>|z|)")]
```

From this table we can conclude that the number of years the country was ruled by a military oligarchy and the number of legal political parties is associated with an increase in the number of military coups. Also, the political liberalization is inversely related to the number of military coups.

## b)
We use the step-down approach to reduce the number of explanatory variables in the model. We removed variables with the lowest p-value sequentially until all p-values were lower than $\alpha=0.05$. 
```{r}
summary(model)$coefficients[, "Pr(>|z|)"]
```
We remove numelec.
```{r}
model <- glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim,family=poisson,data=data)
summary(model)$coefficients[, "Pr(>|z|)"]
```
We remove numregim.
```{r}
model <- glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size,family=poisson,data=data)
summary(model)$coefficients[, "Pr(>|z|)"]
```
We remove size.
```{r}
model <- glm(miltcoup~oligarchy+pollib+parties+pctvote+popn,family=poisson,data=data)
summary(model)$coefficients[, "Pr(>|z|)"]
```
We remove popn.
```{r}
model <- glm(miltcoup~oligarchy+pollib+parties+pctvote,family=poisson,data=data)
summary(model)$coefficients[, "Pr(>|z|)"]
```
The highest p-value is for pollib1, however this is one level of the entire factor pollib which is significant. We remove the variable with the next biggest p-value, pctvote.
```{r}
model <- glm(miltcoup~oligarchy+pollib+parties,family=poisson,data=data)
summary(model)$coefficients[,c("Estimate","Pr(>|z|)")]
```
We stop iterating because there are no more insignificant variables in the model. The remaining variables are the same variables which were significant in the original full model. However this model is better since it contains less variables. The p-value for oligarchy is smaller in comparison to the p-value in the full model, so there is even more certainty about the influence of oligarchy on the number of military coups. 

## c)
To calculate the predicted number of military coups of an hypothetical average country seperated for political liberalization we perform the following r code.
```{r}
countries <- data.frame(oligarchy=mean(data$oligarchy),pollib=as.factor(c(0,1,2)),parties=mean(data$parties),pctvote=mean(data$pctvote),popn=mean(data$popn),size=mean(data$size),numelec=mean(data$numelec),numregim=mean(data$numregim))
predict(model,countries,interval="prediction")
```
This result tells us that an average country with no civil rights for political expression is expected to have approximately one successful military coup between independence and 1989. From this prediction a trend is visible with the number of coups decreasing as the political liberties increase. For the country with full political liberties the number of succesful coups is negative. This is hard to interpret but in practice this would come down to no coups.
